
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<script async src="./analytics.js"></script>
<script type="text/javascript" src="./hidebib.js"></script>
<title>Haoran MO | 莫浩然 's Papers</title>
<!--<link rel="shortcut icon" href="http://cg.cs.tsinghua.edu.cn/people/~xianying/favicon.ico">-->
<style type="text/css">
body {
	margin-top: 30px;
	margin-bottom: 30px;
	margin-left: 100px;
	margin-right: 100px;
}
p {
	margin-top: 0px;
	margin-bottom: 0px;
}
span {
	margin-top: 0px;
	margin-bottom: 0px;
	display: inline-block;
}

.caption {
	font-size: 34px;
	font-weight: normal;
	color: #000;
	font-family: Constantia, "Lucida Bright", "DejaVu Serif", Georgia, serif;
}
.caption-1 {
	font-size: 34px;
	font-weight: normal;
	color: #000;
	font-family: 华文新魏;
}
.caption-2 {
	font-size: 16px;
	font-family: Tahoma, Geneva, sans-serif;
	font-weight: bold;
	color: #990000;
}
.caption-3 {
	font-size: 16px;
	font-family: Tahoma, Geneva, sans-serif;
	font-weight: bold;
	color: #F00;
}
.caption-4 {
	font-size: 16px;
	font-family: Tahoma, Geneva, sans-serif;
	color: #990000;
}
.content {
    margin-top: 4px;
	font-size: 18px;
	font-family: Tahoma, Geneva, sans-serif;
	text-align: justify;
}
.content a {
	font-size: 18px;
	font-family: Tahoma, Geneva, sans-serif;
	color: #000;
}
.content2 {
	font-size: 14px;
	font-family: Tahoma, Geneva, sans-serif;
	color: #666666;
	text-align: justify;
}
.content2 a {
	font-size: 14px;
	font-family: Tahoma, Geneva, sans-serif;
	color: #666666;
}
.content3 {
    margin:5px 5px 5px 5px;
    padding: 10px;
	font-size: 17px;
	font-family: Tahoma, Geneva, sans-serif;
	text-align: justify;
	border: 1px solid rgba(0,0,0,0.125);
    border-radius: 5px;
}
.content4 {
    margin:10px 5px 5px 5px;
    padding: 10px;
	font-size: 17px;
	font-family: ui-monospace,SFMono-Regular,SF Mono,Menlo,Consolas,Liberation Mono,monospace;
	text-align: justify;
	border: 1px solid rgba(0,0,0,0.125);
    border-radius: 5px;
}
.content-bg {
    margin-top: 4px;
	font-size: 18px;
	font-family: Tahoma, Geneva, sans-serif;
	text-align: justify;
	background-color: #fcf8e3;
}
.content strong a {
	font-size: 18px;
	font-family: Tahoma, Geneva, sans-serif;
	color: #41b6c4;
}

a.mybutton {
    width:auto;
    font-size:18px;
	font-family: Tahoma, Geneva, sans-serif;
	font-weight: bold;
    text-decoration:none;
    text-align:center;
    padding:8px 10px 8px 10px;
    margin:5px 5px 0px 0px;
    color:#41b6c4;
    background-color:#fff;
    border-radius:10px;
    box-shadow: inset 0 0 0 1.5px #41b6c4;
    display: inline-block;
}

    a.mybutton:hover {
        color: #fff !important;
        background-color:#41b6c4;
        text-decoration:none;
    }

heading {
  font-family: Tahoma, Geneva, sans-serif;
  font-size: 18px;
  font-weight: 700
}
.title-small {
	font-size: 28px;
	font-family: Georgia, "Times New Roman", Times, serif;
	color: #000;
}
.title-large {
	font-size: 28px;
	font-family: Georgia, "Times New Roman", Times, serif;
	font-weight: bold;
	color: #000;
}
.title-large a {
	color: #41b6c4;
}
.margin {
	font-size: 10px;
	line-height: 10px;
}
.margin-small {
	font-size: 5px;
	line-height: 5px;
}
.margin-large {
	font-size: 16px;
	line-height: 16px;
}
a:link {
	text-decoration: none;
}
a:visited {
	text-decoration: none;
}
content a:link {
	text-decoration: none;
}
content a:visited {
	text-decoration: none;
}
a:hover {
	text-decoration: underline;
}
a:active {
	text-decoration: underline;
	color: #06F;
}
strong a:active {
	text-decoration: underline;
	color: #06F;
}
.profile {
    border-radius: 50%;
}
</style>
<script async="" src="./files/analytics.js.download"></script><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-53682931-1', 'auto');
  ga('send', 'pageview');

</script></head>



<body>

<table border="0" width="100%">
    <tbody>

        <tr>
        <td width="185"><img class="profile" src="./files/misc/haoran.jpg" border="1" height="250"></td>
        <td width="15"></td>
        <td></td>
        <td>
            <table border="0" width="100%">
                <tbody>
                    <tr height="10">
                    <td colspan="2"></td>
                    </tr>

                    <tr height="40">
                    <td>
                        <table border="0">
                            <tbody>
                            <td>
                              <p class="caption">Haoran MO |</p>
                            </td>
                            <td width="5"></td>
                            <td>
                              <p class="caption-1">莫浩然</p>
                            </td>
                            </tbody>
                        </table>
                    </td>
                    </tr>

                    <tr height="20">
                    <td>
                       <p class="content"><a href="https://cislab.hkust-gz.edu.cn/" class="content">Creative Intelligence and Synergy Lab</a></p>
                       <p class="content"><a href="https://cma.hkust-gz.edu.cn/" class="content">Computational Media and Arts (CMA)</a>, <a href="https://infh.hkust-gz.edu.cn//en" class="content">Information Hub</a></p>
                       <p class="content"><a href="https://www.hkust-gz.edu.cn/" class="content">The Hong Kong University of Science and Technology (Guangzhou)</a></p>
                       <p class="content">Guangzhou, China</p>
                    </td>
                    </tr>

                    <tr height="40">
                    <td>
                        <table border="0" width="100%">
                            <tbody>
                            <tr height="20">
                            <td width="55">
                              <p class="content"><strong>Email: </strong></p></td>
                            <td>
                              <p class="content">haoranmo (at) hkust-gz.edu.cn / mohaor (at) mail2.sysu.edu.cn</p></td>
                            </tr>
                            </tbody>
                        </table>
                    </td>
                    </tr>

                    <tr height="20">
                    <td>
                        <p class="margin">&nbsp;</p>
                        <a href="https://github.com/MarkMoHR" class="mybutton">Github</a>
                        <a href="https://scholar.google.com/citations?user=wIqtF_QAAAAJ" class="mybutton">Google Scholar</a>
                        <a href="files/Resume/Resume_Haoran_MO.pdf" class="mybutton">Resume</a>
<!--                        <p class="content">-->
<!--                          <strong><a href="https://github.com/MarkMoHR">Github</a></strong> |-->
<!--                          <strong><a href="https://scholar.google.com.sg/citations?hl=en&pli=1&user=wIqtF_QAAAAJ">Google Scholar</a></strong> |-->
<!--                          <strong><a href="files/Resume/Resume_Haoran_MO.pdf">Resume</a></strong>-->
<!--                          &lt;!&ndash;<strong><a href="https://blog.csdn.net/qq_33000225">Blog</a></strong> |&ndash;&gt;-->
<!--                          &lt;!&ndash;<strong><a href="./resume/resume.html">CV</a></strong>&ndash;&gt;-->
<!--                        </p>-->
                    </td>
                    </tr>

                    <tr height="20">
                    <td colspan="2"></td>
                    </tr>
                </tbody>
            </table>
        </td>
        </tr>
    </tbody>
</table>
<p class="margin">&nbsp;</p>

<br>

<p id="sect-publications" class="title-large">All Publications &nbsp;<a href="index.html">(☞ Selected Publications)</a></p>
<p align="justify" class="content"><em>'#' indicates equal contribution. '*' indicates corresponding author.</em></p>

<br>
<p class="title-small">2024</p>
<table border="0" cellspacing="10">
    <tr>
        <td width="340" valign="top">
            <a href="https://markmohr.github.io/JoSTC/" class="hoverZoomLink">
                <img src="./files/TOG2024/gunman.gif" alt="tracing" width="340" style="border:1px solid rgb(200,200,200)">
            </a>
        </td>
        <td width="1200" valign="middle">
            <heading>Joint Stroke Tracing and Correspondence for 2D Animation</heading>
            <br>
            <p class="margin">&nbsp;</p>
            <p class="content">
                <strong>Haoran Mo</strong>,
                <a href="http://cse.sysu.edu.cn/content/2537">Chengying Gao*</a> and
                <a href="https://sse.sysu.edu.cn/teacher/689">Ruomei Wang</a>
            </p>
            <p class="margin-small">&nbsp;</p>
            <span class="content-bg"><em>ACM Transactions on Graphics (Presented at <strong>SIGGRAPH 2024</strong>)</em></span>
            <em><strong><font color="#F56A6A">(CCF-A)</font></strong></em>
            <p class="margin">&nbsp;</p>

            <div class="paper" id="TOG24">
            <a href="https://markmohr.github.io/JoSTC/" class="mybutton">Project Page</a>
            <a href="https://www.sysu-imsl.com/files/TOG2024/SketchTracing_TOG2024_personal.pdf" class="mybutton">Paper</a>
            <a href="files/TOG2024/SketchTracing_TOG2024_supplemental.pdf" class="mybutton">Supplementary</a>
            <a href="https://github.com/MarkMoHR/JoSTC" class="mybutton">Code</a>
            <a href="javascript:toggleblock('TOG_24')" class="mybutton">Abstract</a>
            <a href="javascript:togglebib('TOG24')" class="mybutton">Bibtex</a>
            <p class="margin">&nbsp;</p>
            <p class="content3" id="TOG_24">
                To alleviate human labor in redrawing keyframes with ordered vector strokes for automatic inbetweening,
                we for the first time propose a joint stroke tracing and correspondence approach.
                Given consecutive raster keyframes along with a single vector image of the starting frame as a guidance,
                the approach generates vector drawings for the remaining keyframes while ensuring one-to-one stroke correspondence.
                Our framework trained on clean line drawings generalizes to rough sketches
                and the generated results can be imported into inbetweening systems to produce inbetween sequences.
                Hence, the method is compatible with standard 2D animation workflow.
                An adaptive spatial transformation module (ASTM) is introduced to handle non-rigid motions and stroke distortion.
                We collect a dataset for training, with 10k+ pairs of raster frames and their vector drawings with stroke correspondence.
                Comprehensive validations on real clean and rough animated frames manifest the effectiveness of our method and superiority to existing methods.
            </p>
            <pre xml:space="preserve" class="content4">
@article{mo2024joint,
  title   = {Joint Stroke Tracing and Correspondence for 2D Animation},
  author  = {Mo, Haoran and Gao, Chengying and Wang, Ruomei},
  journal = {ACM Transactions on Graphics (TOG)},
  year    = {2024}
}
</pre>
            </div>
        </td>
    </tr>

    <tr>
        <td width="340" valign="top">
            <a href="https://github.com/MarkMoHR/DiffSketchEdit" class="hoverZoomLink">
                <img src="./files/ICME2024/editing.gif" alt="icme2024" width="340" style="border:1px solid rgb(200,200,200)">
            </a>
        </td>
        <td width="1200" valign="middle">
            <heading>Text-based Vector Sketch Editing with Image Editing Diffusion Prior</heading>
            <br>
            <p class="margin">&nbsp;</p>
            <p class="content">
                <strong>Haoran Mo</strong>,
                Xusheng Lin,
                <a href="http://cse.sysu.edu.cn/content/2537">Chengying Gao*</a> and
                <a href="https://sse.sysu.edu.cn/teacher/689">Ruomei Wang</a>
            </p>
            <p class="margin-small">&nbsp;</p>
            <span class="content-bg">
                <em>IEEE International Conference on Multimedia & Expo (<strong>ICME 2024</strong>)</em>
            </span>
            <em><strong><font color="#F56A6A">(CCF-B)</font></strong></em>
            <p class="margin">&nbsp;</p>

            <div class="paper" id="ICME24">
            <a href="https://www.sysu-imsl.com/files/ICME2024/ICME2024_sketch_editing_final.pdf" class="mybutton">Paper</a>
            <a href="files/ICME2024/ICME2024_sketch_editing_supp.pdf" class="mybutton">Supplementary</a>
            <a href="https://github.com/MarkMoHR/DiffSketchEdit" class="mybutton">Code</a>
            <a href="javascript:toggleblock('ICME_24')" class="mybutton">Abstract</a>
            <a href="javascript:togglebib('ICME24')" class="mybutton">Bibtex</a>
            <p class="margin">&nbsp;</p>
            <p class="content3" id="ICME_24">
                We present a framework for text-based vector sketch editing to improve the efficiency of graphic design. The key idea behind the approach is to transfer the prior information from raster-level diffusion models, especially those from image editing methods, into the vector sketch-oriented task. The framework presents three editing modes and allows iterative editing. To meet the editing requirement of modifying the intended parts only while avoiding changing the other strokes, we introduce a stroke-level local editing scheme that automatically produces an editing mask reflecting locally editable regions and modifies strokes within the regions only. Comparisons with existing methods demonstrate the superiority of our approach.
            </p>
            <pre xml:space="preserve" class="content4">
@inproceedings{mo2024text,
  title={Text-based Vector Sketch Editing with Image Editing Diffusion Prior},
  author={Mo, Haoran and Lin, Xusheng and Gao, Chengying and Wang, Ruomei},
  booktitle={2024 IEEE International Conference on Multimedia and Expo (ICME)},
  pages={1--6},
  year={2024},
  organization={IEEE}
}
</pre>
            </div>
        </td>
    </tr>

    <tr>
        <td width="340" valign="top">
            <a href="" class="hoverZoomLink">
                <img src="./files/ICME2024/animation.png" alt="icme2024" width="340" style="border:1px solid rgb(200,200,200)">
            </a>
        </td>
        <td width="1200" valign="middle">
            <heading>Video-Driven Sketch Animation via Cyclic Reconstruction Mechanism</heading>
            <br>
            <p class="margin">&nbsp;</p>
            <p class="content">
                Zhuo Xie,
                <strong>Haoran Mo</strong> and
                <a href="http://cse.sysu.edu.cn/content/2537">Chengying Gao*</a>
            </p>
            <p class="margin-small">&nbsp;</p>
            <span class="content-bg">
                <em>IEEE International Conference on Multimedia & Expo (<strong>ICME 2024</strong>)</em>
            </span>
            <em><strong><font color="#F56A6A">(CCF-B)</font></strong></em>
            <p class="margin">&nbsp;</p>

            <div class="paper" id="ICME24_ani">
            <a href="https://www.sysu-imsl.com/files/ICME2024/ICME2024_sketch_animation_final.pdf" class="mybutton">Paper</a>
            <a href="javascript:toggleblock('ICME_24_ani')" class="mybutton">Abstract</a>
            <a href="javascript:togglebib('ICME24_ani')" class="mybutton">Bibtex</a>
            <p class="margin">&nbsp;</p>
            <p class="content3" id="ICME_24_ani">
                Considering the time-consuming manual workflow in 2D sketch animation production, we present an automatic solution by using videos as reference to animate the static sketch images. This includes motion extraction from the videos and injection into the sketches to produce animated sketch sequences in which appearance properties from the source sketches should be preserved. To reduce blurry artifact caused by complex motions and maintain stroke line continuity, we propose to incorporate inner masks of the sketches as an explicit guidance to indicate inner regions and ensure component integrality. Moreover, to bridge the domain gap between the video frames and the sketches when modelling the motions, we introduce a cyclic reconstruction mechanism to increase compatibility with different domains and improve motion consistency between the sketch animation and the driving video.
            </p>
            <pre xml:space="preserve" class="content4">
@inproceedings{xie2024video,
  title={Video-Driven Sketch Animation via Cyclic Reconstruction Mechanism},
  author={Xie, Zhuo and Mo, Haoran and Gao, Chengying},
  booktitle={2024 IEEE International Conference on Multimedia and Expo (ICME)},
  pages={1--6},
  year={2024},
  organization={IEEE}
}
</pre>
            </div>
        </td>
    </tr>

    <tr>
        <td width="340" valign="top">
            <a href="" class="hoverZoomLink">
                <img src="./files/PG2024/pg2024.jpg" width="340" style="border:1px solid rgb(200,200,200)">
            </a>
        </td>
        <td width="1200" valign="middle">
            <heading>Controllable Anime Image Editing via Probability of Attribute Tags</heading>
            <br>
            <p class="margin">&nbsp;</p>
            <p class="content">
                Zhenghao Song,
                <strong>Haoran Mo</strong> and
                <a href="http://cse.sysu.edu.cn/content/2537">Chengying Gao*</a>
            </p>
            <p class="margin-small">&nbsp;</p>
            <span class="content-bg"><em>Computer Graphics Forum (<strong>Pacific Graphics 2024</strong>)</em></span>
            <em><strong><font color="#F56A6A">(CCF-B)</font></strong></em>
            <p class="margin">&nbsp;</p>

            <div class="paper" id="PG2024">
            <a href="" class="mybutton">Paper</a>
            <a href="https://github.com/Songdp-zh/Controllable-Anime-Image-Editing" class="mybutton">Code</a>
            <a href="javascript:toggleblock('PG_24')" class="mybutton">Abstract</a>
            <a href="javascript:togglebib('PG2024')" class="mybutton">Bibtex</a>
            <p class="margin">&nbsp;</p>
            <p class="content3" id="PG_24">
                Editing anime images via probabilities of attribute tags allows controlling the degree of the manipulation in an intuitive and convenient manner.
                Existing methods fall short in the progressive modification and preservation of unintended regions in the input image.
                We propose a controllable anime image editing framework based on adjusting the tag probabilities,
                in which a probability encoding network (PEN) is developed to encode the probabilities into features that capture continuous characteristic of the probabilities.
                Thus, the encoded features are able to direct the generative process of a pre-trained diffusion model and facilitate the linear manipulation.
                We also introduce a local editing module that automatically identifies the intended regions and constrains the edits to be applied to those regions only,
                which preserves the others unchanged.
                Comprehensive comparisons with existing methods indicate the effectiveness of our framework in both one-shot and linear editing modes.
                Results in additional applications further demonstrate the generalization ability of our approach.
            </p>
            <pre xml:space="preserve" class="content4">
@inproceedings{song2024controllable,
  title={Controllable Anime Image Editing via Probability of Attribute Tags},
  author={Song, Zhenghao and Mo, Haoran and Gao, Chengying},
  booktitle={Pacific Graphics},
  year={2024}
}
</pre>
            </div>
        </td>
    </tr>

</table>


<p class="title-small">2023</p>
<table border="0" cellspacing="10">
    <tr>
        <td width="340" valign="top">
            <a href="https://onlinelibrary.wiley.com/doi/10.1111/cgf.14938" class="hoverZoomLink">
                <img src="./files/PG2023/pg2023.jpg" width="340" style="border:1px solid rgb(200,200,200)">
            </a>
        </td>
        <td width="1200" valign="middle">
            <heading>Controllable Garment Image Synthesis Integrated with Frequency Domain Features</heading>
            <br>
            <p class="margin">&nbsp;</p>
            <p class="content">
                Xinru Liang,
                <strong>Haoran Mo</strong> and
                <a href="http://cse.sysu.edu.cn/content/2537">Chengying Gao*</a>
            </p>
            <p class="margin-small">&nbsp;</p>
            <span class="content-bg"><em>Computer Graphics Forum (<strong>Pacific Graphics 2023</strong>)</em></span>
            <em><strong><font color="#F56A6A">(CCF-B)</font></strong></em>
            <p class="margin">&nbsp;</p>

            <div class="paper" id="PG2023">
            <a href="https://onlinelibrary.wiley.com/doi/10.1111/cgf.14938" class="mybutton">Paper</a>
            <a href="javascript:toggleblock('PG_23')" class="mybutton">Abstract</a>
            <a href="javascript:togglebib('PG2023')" class="mybutton">Bibtex</a>
            <p class="margin">&nbsp;</p>
            <p class="content3" id="PG_23">
                Using sketches and textures to synthesize garment images is able to conveniently display the realistic visual effect in the design phase,
                which greatly increases the efficiency of fashion design.
                Existing garment image synthesis methods from a sketch and a texture tend to fail in working on complex textures,
                especially those with periodic patterns.
                We propose a controllable garment image synthesis framework that takes as inputs an outline sketch and a texture patch
                and generates garment images with complicated and diverse texture patterns.
                To improve the performance of global texture expansion, we exploit the frequency domain features in the generative process,
                which are from a Fast Fourier Transform (FFT) and able to represent the periodic information of the patterns.
                We also introduce a perceptual loss in the frequency domain to measure the similarity of two texture pattern patches
                in terms of their intrinsic periodicity and regularity.
                Comparisons with existing approaches and sufficient ablation studies demonstrate the effectiveness of our method
                that is capable of synthesizing impressive garment images with diverse texture patterns while guaranteeing proper texture expansion and pattern consistency.
            </p>
            <pre xml:space="preserve" class="content4">
@inproceedings{liang2023controllable,
  title={Controllable Garment Image Synthesis Integrated with Frequency Domain Features},
  author={Liang, Xinru and Mo, Haoran and Gao, Chengying},
  booktitle={Pacific Graphics},
  year={2023}
}
</pre>
            </div>
        </td>
    </tr>

</table>

<p class="title-small">2022</p>
<table border="0" cellspacing="10">
    <tr>
        <td width="340" valign="top">
            <a href="https://diglib.eg.org/handle/10.2312/pg20221238" class="hoverZoomLink">
                <img src="./files/PG2022/pg2022.jpg" width="340" style="border:1px solid rgb(200,200,200)">
            </a>
        </td>
        <td width="1200" valign="middle">
            <heading>Multi-instance Referring Image Segmentation of Scene Sketches based on Global Reference Mechanism</heading>
            <br>
            <p class="margin">&nbsp;</p>
            <p class="content">
                Peng Ling,
                <strong>Haoran Mo</strong> and
                <a href="http://cse.sysu.edu.cn/content/2537">Chengying Gao*</a>
            </p>
            <p class="margin-small">&nbsp;</p>
            <span class="content-bg"><em>Pacific Graphics (<strong>PG 2022</strong>)</em></span>
            <em><strong><font color="#F56A6A">(CCF-B)</font></strong></em>
            <p class="margin">&nbsp;</p>

            <div class="paper" id="PG2022">
            <a href="https://www.sysu-imsl.com/files/PG2022/referring_sketch_segmentation.pdf" class="mybutton">Paper</a>
            <a href="https://github.com/LQY404/GRM-Net" class="mybutton">Code</a>
            <a href="javascript:toggleblock('PG_22')" class="mybutton">Abstract</a>
            <a href="javascript:togglebib('PG2022')" class="mybutton">Bibtex</a>
            <p class="margin">&nbsp;</p>
            <p class="content3" id="PG_22">
                Scene sketch segmentation based on referring expression plays an important role in sketch editing of anime industry.
                While most existing referring image segmentation approaches are designed for the standard task of generating a binary segmentation mask for a single
                or a group of target(s), we think it necessary to equip these models with the ability of multi-instance segmentation.
                To this end, we propose GRM-Net, a one-stage framework tailored for multi-instance referring image segmentation of scene sketches.
                We extract the language features from the expression and fuse it into a conventional instance segmentation pipeline for filtering out the undesired instances
                in a coarse-to-fine manner and keeping the matched ones.
                To model the relative arrangement of the objects and the relationship among them from a global view,
                we propose a global reference mechanism (GRM) to assign references to each detected candidate to identify its position.
                We compare with existing methods designed for multi-instance referring image segmentation of scene sketches
                and for the standard task of referring image segmentation, and the results demonstrate the effectiveness and superiority of our approach.
            </p>
            <pre xml:space="preserve" class="content4">
@inproceedings{ling2022multi,
  title={Multi-instance Referring Image Segmentation of Scene Sketches based on Global Reference Mechanism},
  author={Ling, Peng and Mo, Haoran and Gao, Chengying},
  booktitle={Pacific Graphics},
  year={2022}
}
</pre>
            </div>
        </td>
    </tr>


    <tr>
        <td width="340" valign="top">
            <a href="https://ieeexplore.ieee.org/abstract/document/9859776" class="hoverZoomLink">
                <img src="./files/ICME2022/icme2022-4.gif" alt="icme2022" width="340" style="border:1px solid rgb(200,200,200)">
            </a>
        </td>
        <td width="1200" valign="middle">
            <heading>Unpaired Motion Style Transfer with Motion-oriented Projection Flow Network</heading>
            <br>
            <p class="margin">&nbsp;</p>
            <p class="content">
                Yue Huang,
                <strong>Haoran Mo</strong>,
                Xiao Liang and
                <a href="http://cse.sysu.edu.cn/content/2537">Chengying Gao*</a>
            </p>
            <p class="margin-small">&nbsp;</p>
            <span class="content-bg">
                <em>IEEE International Conference on Multimedia & Expo (<strong>ICME 2022, Oral</strong>)</em>
            </span>
            <em><strong><font color="#F56A6A">(CCF-B)</font></strong></em>
            <p class="margin">&nbsp;</p>

            <div class="paper" id="ICME22">
            <a href="https://www.sysu-imsl.com/files/ICME2022/Unpaired_Motion_Style_Transfer_camera_ready.pdf" class="mybutton">Paper</a>
            <a href="javascript:toggleblock('ICME_22')" class="mybutton">Abstract</a>
            <a href="javascript:togglebib('ICME22')" class="mybutton">Bibtex</a>
            <p class="margin">&nbsp;</p>
            <p class="content3" id="ICME_22">
                Existing motion style transfer methods trained with unpaired samples tend to generate motions with inconsistent content
                or inconsistent number of frames when compared with the source motion.
                Moreover, due to the limited training samples, these methods perform worse in unseen style.
                In this paper, we propose a novel unpaired motion style transfer framework that generates complete stylized motions with consistent content.
                We introduce a motion-oriented projection flow network (M-PFN) designed for temporal motion data,
                which encodes the content and style motions into latent codes
                and decodes the stylized features produced by adaptive instance normalization (AdaIN) into stylized motions.
                The M-PFN contains dedicated operations and modules, e.g., Transformer, to process the temporal information of motions,
                which help to improve the continuity of the generated motions.
                Comparisons with the state-of-the-art methods show that our method effectively transfers the style of the motions
                while retaining the complete content and has stronger generalization ability in unseen style features.
            </p>
            <pre xml:space="preserve" class="content4">
@inproceedings{huang2022unpaired,
  title={Unpaired Motion Style Transfer with Motion-oriented Projection Flow Network},
  author={Huang, Yue and Mo, Haoran and Liang, Xiao and Gao, Chengying},
  booktitle={2022 IEEE International Conference on Multimedia and Expo (ICME)},
  pages={1--6},
  year={2022},
  organization={IEEE}
}
</pre>
            </div>
        </td>
    </tr>
</table>

<p class="title-small">2021</p>
<table border="0" cellspacing="10">
    <tr>
        <td width="340" valign="top">
            <a href="https://markmohr.github.io/virtual_sketching/" class="hoverZoomLink">
                <img src="./files/SIG2021/muten-fast.gif" alt="vectorization" width="340" style="border:1px solid rgb(200,200,200)">
            </a>
        </td>
        <td width="1200" valign="middle">
            <heading>General Virtual Sketching Framework for Vector Line Art</heading>
            <br>
            <p class="margin">&nbsp;</p>
            <p class="content">
                <strong>Haoran Mo</strong>,
                <a href="https://esslab.jp/~ess/en/">Edgar Simo-Serra</a>,
                <a href="http://cse.sysu.edu.cn/content/2537">Chengying Gao*</a>,
                <a href="https://scholar.google.com/citations?user=kj5HiGgAAAAJ">Changqing Zou</a> and
                <a href="https://sse.sysu.edu.cn/teacher/689">Ruomei Wang</a>
            </p>
            <p class="margin-small">&nbsp;</p>
            <span class="content-bg"><em>ACM Transactions on Graphics (<strong>SIGGRAPH 2021</strong>, Journal track)</em></span>
            <em><strong><font color="#F56A6A">(CCF-A)</font></strong></em>
            <p class="margin">&nbsp;</p>

            <div class="paper" id="SIG21">
            <a href="https://markmohr.github.io/virtual_sketching/" class="mybutton">Project Page</a>
            <a href="https://esslab.jp/publications/HaoranSIGGRAPH2021.pdf" class="mybutton">Paper</a>
            <a href="files/SIG2021/SketchVectorization_SIG2021_supplemental.pdf" class="mybutton">Supplementary</a>
            <a href="https://github.com/MarkMoHR/virtual_sketching" class="mybutton">Code</a>
            <a href="javascript:toggleblock('SIG_21')" class="mybutton">Abstract</a>
            <a href="javascript:togglebib('SIG21')" class="mybutton">Bibtex</a>
            <p class="margin">&nbsp;</p>
            <p class="content3" id="SIG_21">
                Vector line art plays an important role in graphic design, however, it is tedious to manually create.
                We introduce a general framework to produce line drawings from a wide variety of images,
                by learning a mapping from raster image space to vector image space.
                Our approach is based on a recurrent neural network that draws the lines one by one.
                A differentiable rasterization module allows for training with only supervised raster data.
                We use a dynamic window around a virtual pen while drawing lines,
                implemented with a proposed aligned cropping and differentiable pasting modules.
                Furthermore, we develop a stroke regularization loss that encourages the model
                to use fewer and longer strokes to simplify the resulting vector image.
                Ablation studies and comparisons with existing methods corroborate the efficiency of our approach
                which is able to generate visually better results in less computation time,
                while generalizing better to a diversity of images and applications.
            </p>
            <pre xml:space="preserve" class="content4">
@article{mo2021virtualsketching,
  title   = {General Virtual Sketching Framework for Vector Line Art},
  author  = {Mo, Haoran and Simo-Serra, Edgar and Gao, Chengying and Zou, Changqing and Wang, Ruomei},
  journal = {ACM Transactions on Graphics (TOG)},
  year    = {2021},
  volume  = {40},
  number  = {4},
  pages   = {51:1--51:14}
}
</pre>
            </div>
        </td>
    </tr>

    <tr>
        <td width="340" valign="top">
            <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/cgf.14396" class="hoverZoomLink">
                <img src="./files/PG2021/colorization-PG2021.png" alt="colorization-PG2021" width="340" style="border:1px solid rgb(200,200,200)">
            </a>
        </td>
        <td width="1200" valign="middle">
            <heading>Line Art Colorization Based on Explicit Region Segmentation</heading>
            <br>
            <p class="margin">&nbsp;</p>
            <p class="content">
                <a href="https://github.com/Ricardo-L-C">Ruizhi Cao</a>,
                <strong>Haoran Mo</strong> and
                <a href="http://cse.sysu.edu.cn/content/2537">Chengying Gao*</a>
            </p>
            <p class="margin-small">&nbsp;</p>
            <span class="content-bg"><em>Computer Graphics Forum (<strong>Pacific Graphics 2021</strong>)</em></span>
            <em><strong><font color="#F56A6A">(CCF-B)</font></strong></em>
            <p class="margin">&nbsp;</p>

            <div class="paper" id="PG2021">
            <a href="https://www.sysu-imsl.com/files/PG2021/line_art_colorization_pg2021_main.pdf" class="mybutton">Paper</a>
            <a href="https://www.sysu-imsl.com/files/PG2021/line_art_colorization_pg2021_supp.pdf" class="mybutton">Supplementary</a>
            <a href="https://github.com/Ricardo-L-C/ColorizationWithRegion" class="mybutton">Code</a>
            <a href="javascript:toggleblock('PG_21')" class="mybutton">Abstract</a>
            <a href="javascript:togglebib('PG2021')" class="mybutton">Bibtex</a>
            <p class="margin">&nbsp;</p>
            <p class="content3" id="PG_21">
                Automatic line art colorization plays an important role in anime and comic industry.
                While existing methods for line art colorization are able to generate plausible colorized results,
                they tend to suffer from the color bleeding issue.
                We introduce an explicit segmentation fusion mechanism to aid colorization frameworks in avoiding color bleeding artifacts.
                This mechanism is able to provide region segmentation information for the colorization process explicitly
                so that the colorization model can learn to avoid assigning the same color across regions with different semantics
                or inconsistent colors inside an individual region.
                The proposed mechanism is designed in a plug-and-play manner,
                so it can be applied to a diversity of line art colorization frameworks with various kinds of user guidances.
                We evaluate this mechanism in tag-based and reference-based line art colorization tasks
                by incorporating it into the state-of-the-art models.
                Comparisons with these existing models corroborate the effectiveness of our method which largely alleviates the color bleeding artifacts.
            </p>
            <pre xml:space="preserve" class="content4">
@inproceedings{cao2021line,
  title={Line Art Colorization Based on Explicit Region Segmentation},
  author={Cao, Ruizhi and Mo, Haoran and Gao, Chengying},
  booktitle={Computer Graphics Forum},
  volume={40},
  number={7},
  year={2021},
  organization={Wiley Online Library}
}
</pre>
            </div>
        </td>
    </tr>
</table>

<p class="title-small">2019</p>
<table border="0" cellspacing="10">
    <tr>
        <td width="340" valign="top">
            <a href="https://sketchyscene.github.io/SketchySceneColorization/" class="hoverZoomLink">
                <img src="./files/SIGA19/colorization2.png" alt="colorization" width="340" style="border:1px solid rgb(200,200,200)">
            </a>
        </td>
        <td width="1200" valign="middle">
            <heading>Language-based Colorization of Scene Sketches</heading>
            <br>
            <p class="margin">&nbsp;</p>
            <p class="content">
                <a href="https://scholar.google.com/citations?user=kj5HiGgAAAAJ">Changqing Zou<sup>#</sup></a>,
                <strong>Haoran Mo<sup>#</sup></strong>(equal contribution),
                <a href="http://cse.sysu.edu.cn/content/2537">Chengying Gao*</a>,
                <a href="http://www.duruofei.com/">Ruofei Du</a> and
                <a href="https://hongbofu.people.ust.hk/">Hongbo Fu</a>
            </p>
            <p class="margin-small">&nbsp;</p>
            <span class="content-bg"><em>ACM Transactions on Graphics (<strong>SIGGRAPH Asia 2019</strong>, Journal track)</em></span>
            <em><strong><font color="#F56A6A">(CCF-A)</font></strong></em>
            <p class="margin">&nbsp;</p>

            <div class="paper" id="SIGA19">
            <a href="https://sketchyscene.github.io/SketchySceneColorization/" class="mybutton">Project Page</a>
            <a href="files/SIGA19/SketchColorization_paper_SA2019.pdf" class="mybutton">Paper</a>
            <a href="files/SIGA19/SketchColorization_supplementary_SA2019.pdf" class="mybutton">Supplementary</a>
            <a href="https://github.com/SketchyScene/SketchySceneColorization" class="mybutton">Code</a>
            <a href="files/SIGA19/SA2019_SketchColorization_355.pptx" class="mybutton">Slide</a>
            <a href="javascript:toggleblock('SIGA_19')" class="mybutton">Abstract</a>
            <a href="javascript:togglebib('SIGA19')" class="mybutton">Bibtex</a>
            <p class="margin">&nbsp;</p>
            <p class="content3" id="SIGA_19">
                Being natural, touchless, and fun-embracing, language-based inputs have been demonstrated effective
                for various tasks from image generation to literacy education for children. This paper for the first
                time presents a language-based system for interactive colorization of scene sketches, based on semantic
                comprehension. The proposed system is built upon deep neural networks trained on a large-scale
                repository of scene sketches and cartoon-style color images with text descriptions. Given a scene
                sketch, our system allows users, via language-based instructions, to interactively localize and
                colorize specific foreground object instances to meet various colorization requirements in a
                progressive way. We demonstrate the effectiveness of our approach via comprehensive experimental
                results including alternative studies, comparison with the state-of-the-art methods, and generalization
                user studies. Given the unique characteristics of language-based inputs, we envision a combination of
                our interface with a traditional scribble-based interface for a practical multimodal colorization
                system, benefiting various applications. </p>
            <pre xml:space="preserve" class="content4">
@article{zouSA2019sketchcolorization,
  title   = {Language-based Colorization of Scene Sketches},
  author  = {Zou, Changqing and Mo, Haoran and Gao, Chengying and Du, Ruofei and Fu, Hongbo},
  journal = {ACM Transactions on Graphics (TOG)},
  year    = {2019},
  volume  = {38},
  number  = {6},
  pages   = {233:1--233:16}
}
</pre>
            </div>
        </td>
    </tr>
</table>

<p class="title-small">2018</p>
<table border="0" cellspacing="10">
    <tr>
        <td width="340" valign="top">
            <a href="https://sketchyscene.github.io/SketchyScene/" class="hoverZoomLink">
                <img src="./files/ECCV18/sketchyscene2.png" alt="SketchyScene_eccv18" width="340" style="border:1px solid rgb(200,200,200)">
            </a>
        </td>
        <td width="1200" valign="middle">
            <heading>SketchyScene: Richly-Annotated Scene Sketches</heading>
            <br>
            <p class="margin">&nbsp;</p>
            <p class="content">
                <a href="https://scholar.google.com/citations?user=kj5HiGgAAAAJ">Changqing Zou<sup>#</sup></a>,
                <a href="https://yuqian1023.github.io/">Qian Yu<sup>#</sup></a>,
                <a href="http://www.duruofei.com/">Ruofei Du</a>,
                <strong>Haoran Mo</strong>,
                <a href="https://scholar.google.co.uk/citations?user=irZFP_AAAAAJ&hl=en">Yi-Zhe Song</a>,
                <a href="https://scholar.google.co.uk/citations?user=MeS5d4gAAAAJ&hl=en">Tao Xiang</a>,
                <a href="http://cse.sysu.edu.cn/content/2537">Chengying Gao</a>,
                <a href="http://cfcs.pku.edu.cn/baoquan/">Baoquan Chen*</a> and
                <a href="http://www.cs.sfu.ca/~haoz/">Hao Zhang</a>
            </p>
            <p class="margin-small">&nbsp;</p>
            <span class="content-bg"><em>European Conference on Computer Vision (<strong>ECCV 2018</strong>)</em></span>
            <em><strong><font color="#F56A6A">(CCF-B)</font></strong></em>
            <p class="margin">&nbsp;</p>

            <div class="paper" id="SketchyScene_eccv">
            <a href="https://sketchyscene.github.io/SketchyScene/" class="mybutton">Project Page</a>
            <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Changqing_Zou_SketchyScene_Richly-Annotated_Scene_ECCV_2018_paper.pdf" class="mybutton">Paper</a>
            <a href="https://changqingzou.weebly.com/uploads/5/9/3/6/59369643/eccv_2018_qian_changqing_v2.4.pdf" class="mybutton">Poster</a>
            <a href="https://github.com/SketchyScene/SketchyScene" class="mybutton">Code</a>
            <a href="javascript:toggleblock('SketchyScene_eccv18')" class="mybutton">Abstract</a>
            <a href="javascript:togglebib('SketchyScene_eccv')" class="mybutton">Bibtex</a>
            <p class="margin">&nbsp;</p>
            <p class="content3" id="SketchyScene_eccv18"> We contribute the first large-scale dataset of scene
                sketches, SketchyScene, with the goal of advancing research on sketch understanding at both the object
                and scene level. The dataset is created through a novel and carefully designed crowdsourcing pipeline,
                enabling users to efficiently generate large quantities realistic and diverse scene sketches.
                SketchyScene contains more than 29,000 scene-level sketches, 7,000+ pairs of scene templates and photos,
                and 11,000+ object sketches. All objects in the scene sketches have ground-truth semantic and instance
                masks. The dataset is also highly scalable and extensible, easily allowing augmenting and/or changing
                scene composition. We demonstrate the potential impact of SketchyScene by training new computational
                models for semantic segmentation of scene sketches and showing how the new dataset enables several
                applications including image retrieval, sketch colorization, editing, and captioning, etc. </p>
            <pre xml:space="preserve" class="content4">
@inproceedings{zou2018sketchyscene,
  title={Sketchyscene: Richly-annotated scene sketches},
  author={Zou, Changqing and Yu, Qian and Du, Ruofei and Mo, Haoran and Song, Yi-Zhe and Xiang, Tao and Gao, Chengying and Chen, Baoquan and Zhang, Hao},
  booktitle={Proceedings of the european conference on computer vision (ECCV)},
  pages={421--436},
  year={2018}
}
</pre>
            </div>
        </td>
    </tr>
</table>

<br>
<br>

<!--<div id="copyright">-->
<!--    <p class="content2">-->
<!--        Copyright © 2018-2021 Haoran MO |-->
<!--        <a href="http://beian.miit.gov.cn/">粤ICP备2020115736号</a> |-->
<!--        <a href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=44011302002733">粤公网安备 44011302002733号</a>-->
<!--    </p>-->

<!--</div>-->


<!--<div style="width:300px;margin:0 auto; padding:20px 0;">-->
    <!--<a target="_blank" href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=44011302002733" style="display:inline-block;text-decoration:none;height:20px;line-height:20px;"><img src="" style="float:left;"/><p style="float:left;height:20px;line-height:20px;margin: 0px 0px 0px 5px; color:#939393;">粤公网安备 44011302002733号</p></a>-->

    <!--<a target="_blank" href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=44011302002733" style="display:inline-block;text-decoration:none;height:20px;line-height:20px;"><img src="" style="float:left;"/><p style="float:left;height:20px;line-height:20px;margin: 0px 0px 0px 5px; color:#939393;">粤公网安备 44011302002733号</p></a>-->
<!--</div>-->

<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('SketchyScene_eccv18');
hideblock('SIGA_19');
hideblock('SIG_21');
hideblock('PG_21');
hideblock('PG_22');
hideblock('ICME_22');
hideblock('PG_23');
hideblock('TOG_24');
hideblock('ICME_24');
hideblock('ICME_24_ani');
hideblock('PG_24');
</script>




<div style="display:none">
<!-- GoStats JavaScript Based Code -->
<script type="text/javascript" src="./files/counter.js.download"></script><script language="javascript">var _go_js="1.0";</script><script language="javascript1.1">_go_js="1.1";</script><script language="javascript1.2">_go_js="1.2";</script><script language="javascript1.3">_go_js="1.3";</script><script language="javascript1.4">_go_js="1.4";</script><script language="javascript1.5">_go_js="1.5";</script><script language="javascript1.6">_go_js="1.6";</script><script language="javascript1.7">_go_js="1.7";</script><script language="javascript1.8">_go_js="1.8";</script><script language="javascript1.9">_go_js="1.9";</script><script language="javascript"></script>
<script type="text/javascript">_gos='c3.gostats.com';_goa=390583;
_got=4;_goi=1;_goz=0;_god='hits';_gol='web page statistics from GoStats';_GoStatsRun();</script><a target="_blank" href="http://gostats.com/" title="web page statistics from GoStats"><img id="_go_render_39058369" alt="web page statistics from GoStats" title="web page statistics from GoStats" border="0" style="border-width:0px" src="./files/count"></a>
<!-- <a target="_blank" title="web page statistics from GoStats"
href="http://gostats.com"> -->
</div>
<!--
<img alt="web page statistics from GoStats"
src="http://c3.gostats.com/bin/count/a_390583/t_4/i_1/z_0/show_hits/counter.png"
style="border-width:0" />
</a>
-->
<!-- End GoStats JavaScript Based Code -->

</body></html>